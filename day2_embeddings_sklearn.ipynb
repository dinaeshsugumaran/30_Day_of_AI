{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62cabbd8",
   "metadata": {},
   "source": [
    "1. The Symbolic Era (One-Hot, Bag of Words, TF-IDF)\n",
    "\n",
    "Sklearn modules:\n",
    "\n",
    "CountVectorizer → Bag of Words\n",
    "\n",
    "TfidfVectorizer → TF-IDF\n",
    "\n",
    "LabelEncoder, OneHotEncoder → One-hot for categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4fa3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot:\n",
      " <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 4 stored elements and shape (4, 3)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 2)\t1.0\n",
      "Ordinal:\n",
      " [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Example categorical data\n",
    "data = pd.DataFrame({'fruit': ['apple', 'banana', 'apple', 'orange']})\n",
    "\n",
    "# One-hot encoding\n",
    "ohe = OneHotEncoder()\n",
    "onehot = ohe.fit_transform(data[['fruit']])\n",
    "print(\"One-hot:\\n\", onehot)\n",
    "\n",
    "# Ordinal encoding (maps to integers)\n",
    "ord_enc = OrdinalEncoder()\n",
    "ordinal = ord_enc.fit_transform(data[['fruit']])\n",
    "print(\"Ordinal:\\n\", ordinal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76cb0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count vectors:\n",
      " [[0 0 0 1 0 1 1 0]\n",
      " [0 0 1 0 1 1 0 0]\n",
      " [1 1 0 1 1 0 0 1]]\n",
      "TF-IDF vectors:\n",
      " [[0.         0.         0.         0.51785612 0.         0.51785612\n",
      "  0.68091856 0.        ]\n",
      " [0.         0.         0.68091856 0.         0.51785612 0.51785612\n",
      "  0.         0.        ]\n",
      " [0.49047908 0.49047908 0.         0.37302199 0.37302199 0.\n",
      "  0.         0.49047908]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "corpus = [   \"Cats like milk\",\n",
    "    \"Dogs like bones\",\n",
    "    \"Cats and dogs are pets\"]\n",
    "\n",
    "# Bag-of-words (count vectorizer)\n",
    "cv = CountVectorizer()\n",
    "X_count = cv.fit_transform(corpus)\n",
    "print(\"Count vectors:\\n\", X_count.toarray())\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "print(\"TF-IDF vectors:\\n\", X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bce3b0",
   "metadata": {},
   "source": [
    "2. The Statistical Era (LSA, LDA)\n",
    "\n",
    "Sklearn modules:\n",
    "\n",
    "TruncatedSVD → LSA (dimensionality reduction on term-document matrix)\n",
    "\n",
    "LatentDirichletAllocation → LDA topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca1e8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (3, 8)\n",
      "LSA shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Cats like milk\",\n",
    "    \"Dogs like bones\",\n",
    "    \"Cats and dogs are pets\"\n",
    "]\n",
    "\n",
    "# Convert text to TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Apply LSA\n",
    "svd = TruncatedSVD(n_components=2)  # Reduce to 2 topics\n",
    "X_lsa = svd.fit_transform(X)\n",
    "\n",
    "print(\"Original shape:\", X.shape)  # (3 docs x vocab size)\n",
    "print(\"LSA shape:\", X_lsa.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d22bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.71975512 -0.34064651]\n",
      " [ 0.71975512 -0.34064651]\n",
      " [ 0.63428041  0.77310307]]\n"
     ]
    }
   ],
   "source": [
    "print(X_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "269f08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17774eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=2)\n",
    "X_lda = lda.fit_transform(X_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20897bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1491743  0.8508257 ]\n",
      " [0.14917387 0.85082613]\n",
      " [0.90304224 0.09695776]]\n"
     ]
    }
   ],
   "source": [
    "print(X_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75def7",
   "metadata": {},
   "source": [
    "3. The Embedding Era (Word2Vec, GloVe, FastText)\n",
    "\n",
    "Sklearn doesn’t directly implement Word2Vec, but you can use:\n",
    "\n",
    "gensim library for Word2Vec/FastText\n",
    "\n",
    "Or sklearn’s CountVectorizer + TruncatedSVD as a simple approximation of embeddings\n",
    "\n",
    "Example (gensim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3652f08f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      3\u001b[39m sentences = [[\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlove\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcats\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlove\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdogs\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m      4\u001b[39m model = Word2Vec(sentences, vector_size=\u001b[32m50\u001b[39m, window=\u001b[32m2\u001b[39m, min_count=\u001b[32m1\u001b[39m, workers=\u001b[32m4\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"I\", \"love\", \"cats\"], [\"I\", \"love\", \"dogs\"]]\n",
    "model = Word2Vec(sentences, vector_size=50, window=2, min_count=1, workers=4)\n",
    "vector = model.wv['cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc00641c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(gensim.__version__)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50493556",
   "metadata": {},
   "source": [
    "4. The Contextual Era (ELMo, Seq2Seq + Attention)\n",
    "\n",
    "Sklearn cannot do deep contextual embeddings.\n",
    "\n",
    "Use tensorflow.keras or huggingface/transformers for contextual embeddings.\n",
    "\n",
    "You can simulate sequence models with sklearn pipelines on n-grams, but context is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8455a45",
   "metadata": {},
   "source": [
    "5. The Transformer Era (BERT, GPT-2)\n",
    "\n",
    "Sklearn cannot train transformers.\n",
    "\n",
    "Use transformers library from Hugging Face: BertModel, GPT2Model, DistilBERT.\n",
    "\n",
    "Sklearn can still be used on top of embeddings for classification/regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aeb2b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m tokenizer = BertTokenizer.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"I love cats\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "embedding = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cec765",
   "metadata": {},
   "source": [
    "6. The Scale Era (GPT-3/4/5, LLaMA)\n",
    "\n",
    "Access via API (OpenAI, Hugging Face Inference API).\n",
    "\n",
    "Use embeddings as input to sklearn models for downstream tasks: clustering, classification, semantic search.\n",
    "\n",
    "7. The Multimodal Era (CLIP, GPT-4V, Gemini)\n",
    "\n",
    "Sklearn cannot handle multimodal directly.\n",
    "\n",
    "Use OpenAI CLIP, Hugging Face CLIP models, or other vision+text embeddings.\n",
    "\n",
    "Once embeddings are obtained, sklearn can handle clustering, similarity search, classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "30dayai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
